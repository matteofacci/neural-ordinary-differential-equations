# Neural Ordinary Differential Equations
State of the art and implementation of the Neural ODEs for dynamical systems identification.


Neural Networks (NN) is a set of algorithms that allows computers to solve a common problem and recognize patterns, nowadays used in a wide range of tasks, thank the effort made by researchers since the first intuition made by D. O. Hebbo in the book _The Organization of Behaviour_ published in 1949, in which are proposed connection with a complex model of the brain. After that in 1958 J. Von Neumann published _The Computer and the Brain_ in which points out the poor precision that the structures of the models proposed by D. O. Hebb have to perform complex problems. In the same year, Frak Rosenblatt published the first scheme of NN called _Perceptron_, which is the basic element of all the NN used nowadays. This last publication gives inspiration to many researchers for a decade, where some of these researchers were financed by the United State of America until Marvin Minsky and Seymour A. Papert have shown the big limit of these simple structures models. After some years thanks to the works made, first by Paul Werbos in his Ph.D. thesis and then by  J. J. Hopfield in his work where he studies models to recognize very general patterns, the researchers began to study NNs and continue to this day. In these years, one of the most used techniques, called Backpropagation, has been developed to train a NN. It allows adjusting all the weights present in a network systematically in two phases; the forward phase, in which is computed the output of the network and the error with the real value _label_,  and the backward phase in which the error is backpropagated from the output to the input. Moreover, new architectures suitable for specific tasks such as Convolutional Neural Networks (CNN) have been developed, suitable for example for image classification tasks, because they are able to work with very large input and they can extract important features from the images. Another example is represented by Recurrent Neural Networks (RNN) suitable for tasks in which there is a self-correlation among inputs e.g. translation, image captioning, sentiment analysis, etc.
Many other architectures have been developed. 


In particular, recently a new architecture, called Neural Ordinary Differential Equation (NeuralODE) has been developed by Ricky Tian Qi Chen et al. in _NeuralODE_ from the University of Toronto, this paper became prominent after being named one of the best student papers at NeurIPS 2018 in Montreal because it showed a radical new design capable to describe a dynamical system in continuous time. In this way, they give a powerful tool to identify a system and its parameters without using well-known techniques based on System Identification theory.
Therefore, NeuralODEs make a bridge between Artificial Intelligence and dynamical systems, and not only. In fact, they can be used also for simpler tasks such as image classification.


In this project the concept of NeuralODE and its architecture are developed, first focusing on the capability of this Deep Neural Network (DNN) to perform a simple System Identification task, then exploiting it for the classification of electrocardiogram signals (ECG Heartbeat Classification task), also comparing it with another architecture from which it takes shape, ResNet.


Please refer to the reports for a detailed analysis:

* [_Neural Ordinary Differential Equation_](https://github.com/matteofacci/neural-ordinary-differential-equations/blob/fa37732623ad72fc38ec9267af1b0178364ec6f9/reports%20and%20presentation/Neural_Ordinary_Differential_Equation.pdf)
* [_State of the Art: Recurrent Neural Networks for Dynamical Systems Identification_](https://github.com/matteofacci/neural-ordinary-differential-equations/blob/fa37732623ad72fc38ec9267af1b0178364ec6f9/reports%20and%20presentation/State_of_the_Art_Recurrent_Neural_Networks_for_dynamical_systems_identification.pdf)
